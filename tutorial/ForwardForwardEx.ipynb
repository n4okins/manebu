{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward-Forward\n",
    "[pytorch_forward_forward](https://github.com/mohammadpz/pytorch_forward_forward)\n",
    "\n",
    "No use Threshold version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as opt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as tvtf\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from abc import abstractmethod\n",
    "from tqdm.notebook import tqdm\n",
    "from uuid import uuid4\n",
    "from pprint import pprint\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def MNIST_loaders(train_batch_size=50000, test_batch_size=10000):\n",
    "    transform = tvtf.Compose([\n",
    "        tvtf.ToTensor(),\n",
    "        tvtf.Normalize((0.1307,), (0.3081,)),\n",
    "        tvtf.Lambda(lambda x: torch.flatten(x))])\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        MNIST('./ignore_dir/data/', train=True,\n",
    "              download=True,\n",
    "              transform=transform),\n",
    "        batch_size=train_batch_size, shuffle=True)\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        MNIST('./ignore_dir/data/', train=False,\n",
    "              download=True,\n",
    "              transform=transform),\n",
    "        batch_size=test_batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "def overlay_y_on_x(x, y):\n",
    "    x_ = x.clone()\n",
    "    x_[:, :10] *= 0.0\n",
    "    x_[range(x.shape[0]), y] = x.max()\n",
    "    return x_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FFModule(nn.Module):\n",
    "    def __init__(self, layer_train_num_epochs: int):\n",
    "        super().__init__()\n",
    "        self.train_num_epochs = layer_train_num_epochs\n",
    "        self.activation_fn = None\n",
    "        self.optimizer = None\n",
    "\n",
    "    @abstractmethod    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        calc \"Goodness\" value of input data\n",
    "        \"\"\"\n",
    "        return\n",
    "\n",
    "    def train(self, positive, negative):\n",
    "        with tqdm(range(self.train_num_epochs)) as t:\n",
    "\n",
    "            for i in t:\n",
    "                goodness_pos = self.forward(positive).pow(2).mean(1)\n",
    "                goodness_neg = self.forward(negative).pow(2).mean(1)\n",
    "                loss: torch.Tensor = torch.log(\n",
    "                    1 + torch.exp(\n",
    "                        torch.cat([-goodness_pos, goodness_neg])\n",
    "                    )\n",
    "                ).mean()\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "        return self.forward(positive).detach(), self.forward(negative).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FFLinear(FFModule):\n",
    "    __constants__ = ['in_features', 'out_features']\n",
    "    in_features: int\n",
    "    out_features: int\n",
    "    layer_train_num_epochs: int\n",
    "    weight: torch.Tensor\n",
    "\n",
    "    def __init__(self, in_features: int, out_features: int, layer_train_num_epochs: int = 1000,\n",
    "                 bias: bool = True, device=None, dtype=None) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super(FFLinear, self).__init__(layer_train_num_epochs=layer_train_num_epochs)\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.parameter.Parameter(torch.empty((out_features, in_features), **factory_kwargs))\n",
    "        if bias:\n",
    "            self.bias = nn.parameter.Parameter(torch.empty(out_features, **factory_kwargs))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n",
    "            nn.init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        return F.linear(input, self.weight, self.bias)\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return 'in_features={}, out_features={}, bias={}'.format(\n",
    "            self.in_features, self.out_features, self.bias is not None\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FFLayer(FFLinear):\n",
    "    def __init__(self, in_features, out_features, layer_train_num_epochs=100,\n",
    "                 bias=True, device=None, dtype=None):\n",
    "        super().__init__(in_features, out_features, layer_train_num_epochs, bias, device, dtype)\n",
    "        self.activation_fn = nn.ReLU()\n",
    "        self.optimizer = opt.Adam(self.parameters(), lr=0.03)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_direction = x / (x.norm(2, 1, keepdim=True) + 1e-4)\n",
    "        return self.activation_fn(\n",
    "            torch.mm(x_direction, self.weight.T) +\n",
    "            self.bias.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, dims):\n",
    "        super().__init__()\n",
    "        self.layers = []\n",
    "        for d in range(len(dims) - 1):\n",
    "            self.layers += [FFLayer(dims[d], dims[d + 1], layer_train_num_epochs=500).cuda()]\n",
    "\n",
    "    def predict(self, x, tmp=False):\n",
    "        goodness_per_label = []\n",
    "        for label in range(10):\n",
    "            h = overlay_y_on_x(x, label)\n",
    "            goodness = []\n",
    "            for layer in self.layers:\n",
    "                h = layer(h)\n",
    "                goodness += [h.pow(2).mean(1)]\n",
    "            goodness_per_label += [sum(goodness).unsqueeze(1)]\n",
    "\n",
    "        goodness_per_label = torch.cat(goodness_per_label, 1)\n",
    "        goodness_per_label = F.softmax(goodness_per_label, dim=1)\n",
    "\n",
    "        if tmp: pprint(list(enumerate(goodness_per_label[0])))\n",
    "        return goodness_per_label.argmax(1)\n",
    "\n",
    "    def train(self, x_pos, x_neg):\n",
    "        h_pos, h_neg = x_pos, x_neg\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            h_pos, h_neg = layer.train(h_pos, h_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1234)\n",
    "train_loader, test_loader = MNIST_loaders()\n",
    "\n",
    "net = Net([784, 512, 256, 64])\n",
    "x, y = next(iter(train_loader))\n",
    "x, y = x.cuda(), y.cuda()\n",
    "x.size(0), y.size(0)\n",
    "\n",
    "x_pos = overlay_y_on_x(x, y)\n",
    "rnd = torch.randperm(x.size(0))\n",
    "x_neg = overlay_y_on_x(x, y[rnd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/500 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "70f9366862ef4156bfd718a309de2c79"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/500 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ed37a1c6c4d044819119ec41e04b3b48"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/500 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0e27b637c4ba470ea6871d9be600efe6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train error: 0.09102004766464233\n",
      "test error: 0.0886000394821167\n"
     ]
    }
   ],
   "source": [
    "net.train(x_pos, x_neg)\n",
    "\n",
    "print('train error:', 1.0 - net.predict(x).eq(y).float().mean().item())\n",
    "\n",
    "x_te, y_te = next(iter(test_loader))\n",
    "x_te, y_te = x_te.cuda(), y_te.cuda()\n",
    "\n",
    "print('test error:', 1.0 - net.predict(x_te).eq(y_te).float().mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([4742,  365,  556, 3026,  147,  132, 1148, 3004, 1689,  120, 3300,  509]),\n tensor([7, 0, 4, 1, 2, 5, 0, 8, 2, 5, 3, 5], device='cuda:0'),\n tensor([7, 0, 4, 1, 2, 5, 0, 2, 2, 5, 3, 5], device='cuda:0'))"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = torch.randperm(5000)[0:12]\n",
    "k, net.predict(x_te[k]), y_te[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, tensor(0.0786, device='cuda:0', grad_fn=<UnbindBackward0>)),\n",
      " (1, tensor(0.0719, device='cuda:0', grad_fn=<UnbindBackward0>)),\n",
      " (2, tensor(0.1058, device='cuda:0', grad_fn=<UnbindBackward0>)),\n",
      " (3, tensor(0.0677, device='cuda:0', grad_fn=<UnbindBackward0>)),\n",
      " (4, tensor(0.0672, device='cuda:0', grad_fn=<UnbindBackward0>)),\n",
      " (5, tensor(0.0786, device='cuda:0', grad_fn=<UnbindBackward0>)),\n",
      " (6, tensor(0.0822, device='cuda:0', grad_fn=<UnbindBackward0>)),\n",
      " (7, tensor(0.0671, device='cuda:0', grad_fn=<UnbindBackward0>)),\n",
      " (8, tensor(0.3134, device='cuda:0', grad_fn=<UnbindBackward0>)),\n",
      " (9, tensor(0.0677, device='cuda:0', grad_fn=<UnbindBackward0>))]\n",
      "3004 tensor([8], device='cuda:0') tensor(2, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 3004# int(torch.randint(0, 5000, (1, )))\n",
    "print(index, net.predict(x_te[index].unsqueeze(0), tmp=True), y_te[index])\n",
    "plt.imshow(x_te[index].cpu().reshape((1, 28, 28)).numpy().transpose((1, 2, 0)), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def return_default(var, default_value=None):\n",
    "#     return var or default_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class FFModule(nn.Module):\n",
    "#     __layer_cnt__ = 0\n",
    "#     __layer_dict__ = dict()\n",
    "#     def __init__(\n",
    "#         self, train_num_epochs_in_layer: int,\n",
    "#         activation_fn = None, optimizer_fn = None, layer_name = None,\n",
    "#         learning_rate = None,\n",
    "#         **kwargs\n",
    "#     ):\n",
    "#         nn.Module.__init__(self)\n",
    "#\n",
    "#         learning_rate = return_default(learning_rate, 0.001)\n",
    "#         activation_fn = return_default(activation_fn, nn.ReLU)\n",
    "#         self._optimizer_fn = return_default(optimizer_fn, opt.Adam)\n",
    "#\n",
    "#         self.train_num_epochs_in_layer = train_num_epochs_in_layer\n",
    "#         self.activation_fn = activation_fn()\n",
    "#         self.optimizer = None\n",
    "#         self.learning_rate = learning_rate\n",
    "#\n",
    "#         self._name = layer_name or str(uuid4())[:8]\n",
    "#         self._layer_num = FFModule.__layer_cnt__\n",
    "#\n",
    "#         FFModule.__layer_cnt__ += 1\n",
    "#         FFModule.__layer_dict__[self._name] = self\n",
    "#\n",
    "#     def init(self):\n",
    "#         self.init_optimizer()\n",
    "#\n",
    "#     def init_optimizer(self):\n",
    "#         self.optimizer = self._optimizer_fn(self.parameters(), lr=self.learning_rate)\n",
    "#\n",
    "#     @abstractmethod\n",
    "#     def forward(self, x):\n",
    "#         return\n",
    "#\n",
    "#     def calc_loss(self, positive_goodness, negative_goodness) -> torch.Tensor:\n",
    "#         return torch.log(\n",
    "#             1 + torch.exp(\n",
    "#                 torch.cat(\n",
    "#                     [-positive_goodness, negative_goodness]\n",
    "#                 )\n",
    "#             )\n",
    "#         ).mean()\n",
    "#\n",
    "#     def train(self, positive_sample, negative_sample):\n",
    "#         with tqdm(range(self.train_num_epochs_in_layer)) as t:\n",
    "#             for i in t:\n",
    "#                 t.set_description(f\"[training layer {self.__layer_num__}:'{self.name}'][epoch: {i:d4}]\")\n",
    "#                 loss = self.calc_loss(\n",
    "#                     self.forward(positive_sample).pow(2).mean(1),\n",
    "#                     self.forward(negative_sample).pow(2).mean(1),\n",
    "#                 )\n",
    "#                 self.optimizer.zero_grad()\n",
    "#                 loss.backward()\n",
    "#\n",
    "#                 self.optimizer.step()\n",
    "#                 t.update()\n",
    "#\n",
    "#         return self.forward(positive_sample).detach(), self.forward(negative_sample).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class FFConv2d(FFModule, nn.Conv2d):\n",
    "#     def __init__(\n",
    "#         self, in_channels: int, out_channels: int,\n",
    "#         kernel_size, stride = 1, padding = 0,\n",
    "#         dilation = 1, groups: int = 1, bias: bool = True,\n",
    "#         padding_mode: str = 'zeros', device = None, dtype = None,\n",
    "#         train_num_epochs_in_layer: int = 1000,\n",
    "#         activation_fn = None, optimizer_fn = None, learning_rate = None,\n",
    "#         layer_name = None,\n",
    "#         **kwargs\n",
    "#     ) -> None:\n",
    "#         nn.Conv2d.__init__(\n",
    "#             self, in_features=in_features, out_features=out_features,\n",
    "#             bias=bias, device=device, dtype=dtype\n",
    "#         )\n",
    "#         FFModule.__init__(\n",
    "#             self, train_num_epochs_in_layer=train_num_epochs_in_layer,\n",
    "#             activation_fn=activation_fn, optimizer_fn=optimizer_fn, layer_name=layer_name,\n",
    "#             learning_rate=learning_rate\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class FFLayer(FFLinear):\n",
    "#     def __init__(self):\n",
    "#         super().__init__(784, 10)\n",
    "#         self.activation_fn = nn.ReLU()\n",
    "#         self.optimizer = opt.Adam(self.parameters(), lr=0.03)\n",
    "#\n",
    "#         self.init()\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         x_direction = x / (x.norm(2, 1, keepdim=True) + 1e-4)\n",
    "#         return self.activation_fn(\n",
    "#             torch.mm(x_direction, self.weight.T) +\n",
    "#             self.bias.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FFLayer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
