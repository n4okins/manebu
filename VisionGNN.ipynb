{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c473aaa-de60-4f23-9a07-ca4cc573fdbd",
   "metadata": {},
   "source": [
    "# [Vision GNN: An Image is Worth Graph of Nodes](https://ar5iv.labs.arxiv.org/html/2206.00272)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f94bd94e-cfbf-44ed-8cf4-ae476e671a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from typing import Literal, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b5e2818-fd15-4189-926a-0320fa46232e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseDilated(nn.Module):\n",
    "    \"\"\"\n",
    "    Find dilated neighbor from neighbor list\n",
    "    edge_index: (2, batch_size, num_points, k)\n",
    "    \"\"\"\n",
    "    def __init__(self, k=9, dilation=1, stochastic=False, epsilon=0.0):\n",
    "        super(DenseDilated, self).__init__()\n",
    "        self.dilation = dilation\n",
    "        self.stochastic = stochastic\n",
    "        self.epsilon = epsilon\n",
    "        self.k = k\n",
    "\n",
    "    def forward(self, edge_index):\n",
    "        if self.stochastic:\n",
    "            if torch.rand(1) < self.epsilon and self.training:\n",
    "                num = self.k * self.dilation\n",
    "                randnum = torch.randperm(num)[:self.k]\n",
    "                edge_index = edge_index[:, :, :, randnum]\n",
    "            else:\n",
    "                edge_index = edge_index[:, :, :, ::self.dilation]\n",
    "        else:\n",
    "            edge_index = edge_index[:, :, :, ::self.dilation]\n",
    "        return edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05f90a23-6602-46da-b823-26fb7d4f352d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseDilatedKnnGraph(nn.Module):\n",
    "    \"\"\"\n",
    "    Find the neighbors' indices based on dilated knn\n",
    "    \"\"\"\n",
    "    def __init__(self, k=9, dilation=1, stochastic=False, epsilon=0.0):\n",
    "        super(DenseDilatedKnnGraph, self).__init__()\n",
    "        self.dilation = dilation\n",
    "        self.stochastic = stochastic\n",
    "        self.epsilon = epsilon\n",
    "        self.k = k\n",
    "        self._dilated = DenseDilated(k, dilation, stochastic, epsilon)\n",
    "        self.knn = dense_knn_matrix\n",
    "\n",
    "    def forward(self, x):\n",
    "        edge_index = self.knn(x, self.k * self.dilation)\n",
    "        return self._dilated(edge_index)\n",
    "    \n",
    "class DilatedKnnGraph(nn.Module):\n",
    "    \"\"\"\n",
    "    Find the neighbors' indices based on dilated knn\n",
    "    \"\"\"\n",
    "    def __init__(self, k=9, dilation=1, stochastic=False, epsilon=0.0):\n",
    "        super(DilatedKnnGraph, self).__init__()\n",
    "        self.dilation = dilation\n",
    "        self.stochastic = stochastic\n",
    "        self.epsilon = epsilon\n",
    "        self.k = k\n",
    "        self._dilated = DenseDilated(k, dilation, stochastic, epsilon)\n",
    "        self.knn = knn_graph\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.squeeze(-1)\n",
    "        B, C, N = x.shape\n",
    "        edge_index = []\n",
    "        for i in range(B):\n",
    "            edgeindex = self.knn(x[i].contiguous().transpose(1, 0).contiguous(), self.k * self.dilation)\n",
    "            edgeindex = edgeindex.view(2, N, self.k * self.dilation)\n",
    "            edge_index.append(edgeindex)\n",
    "        edge_index = torch.stack(edge_index, dim=1)\n",
    "        return self._dilated(edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17af81e8-a3f6-4bc4-8f72-03da168450d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicConv(nn.Sequential):\n",
    "    def __init__(\n",
    "        self, channels: tuple[int, ...],\n",
    "        act: Literal[\"relu\", \"leakyrelu\", \"prelu\"] = \"relu\",\n",
    "        norm: Optional[Literal[\"batch\", \"instance\"]] = None,\n",
    "        bias: bool = True, drop: float = 0.0\n",
    "    ):\n",
    "        m = []\n",
    "        for i in range(1, len(channels)):\n",
    "            m.append(nn.Conv2d(channels[i - 1], channels[i], 1, bias=bias))\n",
    "            if act is not None and act.lower() != 'none':\n",
    "                m.append(act_layer(act))\n",
    "            if norm is not None and norm.lower() != 'none':\n",
    "                m.append(norm_layer(norm, channels[-1]))\n",
    "            if drop > 0:\n",
    "                m.append(nn.Dropout2d(drop))\n",
    "\n",
    "        super(BasicConv, self).__init__(*m)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.InstanceNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e70d589-9dd0-480e-beeb-d76a8ff513bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRConv2d(nn.Module):\n",
    "    \"\"\"\n",
    "    Max-Relative Graph Convolution (Paper: https://arxiv.org/abs/1904.03751) for dense data type\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, in_channels: int, out_channels: int,\n",
    "        act: Literal[\"relu\", \"leakyrelu\", \"prelu\"] = \"relu\",\n",
    "        norm: Optional[Literal[\"batch\", \"instance\"]] = None,\n",
    "        bias: bool = True\n",
    "    ):\n",
    "        super(MRConv2d, self).__init__()\n",
    "        self.nn = BasicConv([in_channels*2, out_channels], act, norm, bias)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x_i = batched_index_select(x, edge_index[1])\n",
    "        x_j = batched_index_select(x, edge_index[0])\n",
    "        x_j, _ = torch.max(x_j - x_i, -1, keepdim=True)\n",
    "        return self.nn(torch.cat([x, x_j], dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96bb5883-a093-42ae-83d2-0ac415ecb3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EdgeConv2d(nn.Module):\n",
    "    \"\"\"\n",
    "    Edge convolution layer (with activation, batch normalization) for dense data type\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, in_channels: int, out_channels: int,\n",
    "        act: Literal[\"relu\", \"leakyrelu\", \"prelu\"] = \"relu\",\n",
    "        norm: Optional[Literal[\"batch\", \"instance\"]] = None,\n",
    "        bias: bool = True\n",
    "    ):\n",
    "        super(EdgeConv2d, self).__init__()\n",
    "        self.nn = BasicConv([in_channels * 2, out_channels], act, norm, bias)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x_i = batched_index_select(x, edge_index[1])\n",
    "        x_j = batched_index_select(x, edge_index[0])\n",
    "        max_value, _ = torch.max(self.nn(torch.cat([x_i, x_j - x_i], dim=1)), -1, keepdim=True)\n",
    "        return max_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d5f9fc5-381c-4c5c-bb48-d4e7dfb8f45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConv2d(nn.Module):\n",
    "    \"\"\"\n",
    "    Static Graph Convolution Layer\n",
    "    Ref: https://github.com/lightaime/deep_gcns_torch/blob/master/gcn_lib/dense/torch_vertex.py\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, in_channels: int, out_channels: int,\n",
    "        conv: Literal[\"edge\", \"mr\"] = \"edge\",\n",
    "        act: Literal[\"relu\", \"leakyrelu\", \"prelu\"] = \"relu\",\n",
    "        norm: Optional[Literal[\"batch\", \"instance\"]] = None,\n",
    "        bias: bool = True\n",
    "    ):\n",
    "        super(GraphConv2d, self).__init__()\n",
    "        \n",
    "        self.gconv = {\n",
    "            \"edge\": EdgeConv2d(in_channels, out_channels, act, norm, bias),\n",
    "            \"mr\": MRConv2d(in_channels, out_channels, act, norm, bias)\n",
    "        }.get(conv, NotImplementedError(\"\"))\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        return self.gconv(x, edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "919e8807-fb6b-4655-9586-6d3803c5fcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicConv2d(GraphConv2d):\n",
    "    \"\"\"\n",
    "    Dynamic Graph Covolution Layer\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, in_channels, out_channels, \n",
    "        kernel_size: int = 9, dilation =1, \n",
    "        conv='edge', act='relu',\n",
    "        norm=None, bias=True,\n",
    "        stochastic=False, epsilon=0.0, knn='matrix'\n",
    "    ):\n",
    "        super(DynamicConv2d, self).__init__(in_channels, out_channels, conv, act, norm, bias)\n",
    "        self.k = kernel_size\n",
    "        self.d = dilation\n",
    "        if knn == 'matrix':\n",
    "            self.dilated_knn_graph = DenseDilatedKnnGraph(kernel_size, dilation, stochastic, epsilon)\n",
    "        else:\n",
    "            self.dilated_knn_graph = DilatedKnnGraph(kernel_size, dilation, stochastic, epsilon)\n",
    "\n",
    "    def forward(self, x, edge_index=None):\n",
    "        if edge_index is None:\n",
    "            edge_index = self.dilated_knn_graph(x)\n",
    "        return super(DynConv2d, self).forward(x, edge_index)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f77f37d4-3b8b-42a6-98d9-ea3788eaa35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GrapherModule(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels, hidden_channels,\n",
    "        k=9, dilation=1, drop_path=0.0\n",
    "    ):\n",
    "        super(GrapherModule, self).__init__()\n",
    "        self.fc_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels, 1, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(in_channels)\n",
    "        )\n",
    "        \n",
    "        self.graph_conv = nn.Sequential(\n",
    "            DynamicConv2d(in_channels, hidden_channels, k, dilation, act=None),\n",
    "            nn.BatchNorm2d(hidden_channels),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        \n",
    "        self.fc_2 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_channels, in_channels, 1, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(in_channels)\n",
    "        )\n",
    "        \n",
    "        self.drop_path = nn.Identity()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603ab933-97a6-48da-bd4a-0295519e1118",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
